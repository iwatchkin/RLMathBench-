# RLMathBench
This project investigates the impact of Reinforcement Learning from Human Feedback (RLHF) on mathematical reasoning in language models. Using benchmarks like GSM8K, SVAMP, and MATH, we compare model performance before and after RLHF fine-tuning to assess improvements in accuracy and logical reasoning.
